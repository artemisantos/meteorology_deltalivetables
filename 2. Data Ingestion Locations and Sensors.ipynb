{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e54a17-0d8c-40b5-ac04-bf039a1e765f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e91c5d-2dae-411c-a91f-aadf62d7d1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Logging configuration and initialization, reduced for external libraries.\n",
    "'''\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "for lib in [\"azure\", \"urllib3\", \"py4j\"]:\n",
    "    logging.getLogger(lib).setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbedc5ae-2ee4-401c-b1b8-1198b2e18fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fetching access data from Databricks secrets.\n",
    "'''\n",
    "\n",
    "AZURE_CONNECTION_STRING = dbutils.secrets.get(\"dev_secrets\", \"storageconnection\")\n",
    "API_KEY_OPENAQ = dbutils.secrets.get(\"dev_secrets\", \"openaq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58013907-5d11-40e5-8316-fdc5152dafe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initializing the client for handling Azure Blob Storage.\n",
    "'''\n",
    "\n",
    "CONTAINER_NAME = \"data\"\n",
    "blob_service = BlobServiceClient.from_connection_string(AZURE_CONNECTION_STRING)\n",
    "container_client = blob_service.get_container_client(CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d618cf4c-f4d7-4152-8586-e3a6a0bb6aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Definiton of paths, API endpoints and query parameters.\n",
    "'''\n",
    "\n",
    "LOCATIONS_PATH = \"openaq-locations-data\"\n",
    "SENSORS_PATH = \"openaq-sensors-data\"\n",
    "API_LOCATIONS_URL = \"https://api.openaq.org/v3/locations\"\n",
    "API_SENSORS_URL = \"https://api.openaq.org/v3/sensors\"\n",
    "COUNTRY_CODE = \"PL\"\n",
    "PAGE_LIMIT = 1000\n",
    "REQUEST_TIMEOUT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3328d297-783d-4e56-864d-04f3c42b160c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Supported locations and air quality parameters.\n",
    "'''\n",
    "\n",
    "location_ids = {\n",
    "    \"7253\", \"10646\", \"9635\", \"7250\", \"7961\", \"9701\", \"9343\", \"10691\", \"7261\", \"10679\", \"10760\", \"10659\", \"9273\", \"10707\", \"10570\", \"9692\", \"10625\", \"10605\", \"7210\", \"7239\", \"9505\", \"7211\", \"9739\", \"9324\", \"10743\", \"10744\", \"10575\", \"7264\", \"10631\", \"6387\", \"7251\", \"10720\", \"7785\", \"10573\", \"10587\", \"10589\", \"9699\", \"6384\", \"9715\", \"9725\", \"7263\", \"9743\", \"41727\", \"9782\", \"7245\", \"6386\", \"10511\", \"6382\", \"9784\", \"10454\", \"7252\", \"7207\", \"9042\", \"10618\", \"6351\", \"10593\"\n",
    "}\n",
    "air_quality_parameters = {\"pm10\", \"no2\", \"o3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf5c2c4-0d26-4895-b8d7-23eff656282f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_blob_data(path: str):\n",
    "    '''\n",
    "    Retrieves the latest JSON file from Azure Blob Storage.\n",
    "\n",
    "    Args:\n",
    "        path (str): Azure Blob Storage path.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: JSON data or None if no data found.\n",
    "    '''\n",
    "    try:\n",
    "        # Fetch the list of all files (blobs) in the given path.\n",
    "        blob_list = list(container_client.list_blobs(name_starts_with=path))\n",
    "\n",
    "        if not blob_list:\n",
    "            logger.info(f\"No existing data found in Azure Blob Storage for path: {path}\")\n",
    "            return None\n",
    "\n",
    "        # Identify the latest file based on the last modified date.\n",
    "        latest_blob = max(blob_list, key=lambda b: b.last_modified)\n",
    "\n",
    "        blob_client = container_client.get_blob_client(latest_blob.name)\n",
    "\n",
    "        # Fetch the file content as JSON and convert it into a Python dictionary.\n",
    "        blob_data = json.loads(blob_client.download_blob().readall())\n",
    "\n",
    "        logger.info(f\"Latest data file retrieved: {latest_blob.name}\")\n",
    "        return blob_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error retrieving latest blob data from {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7af3cd-4c58-44e5-866f-e9e25be2148c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_location_to_blob(location, is_active=True):\n",
    "    '''\n",
    "    Saves a single location as an individual JSON file in Azure Blob Storage.\n",
    "\n",
    "    Args:\n",
    "        location (dict): Location data to be saved.\n",
    "        is_active (bool, optional): Indicates if the sensor is active. Default=True.\n",
    "    '''\n",
    "    try:\n",
    "        location_id = str(location[\"id\"])\n",
    "        location[\"is_active\"] = is_active\n",
    "\n",
    "        # Format of the file name: \"openaq-locations-data/location_<ID>.json\"\n",
    "        blob_name = f\"{LOCATIONS_PATH}/location_{location_id}.json\"\n",
    "        container_client.upload_blob(blob_name, json.dumps(location, indent=2), overwrite=True)\n",
    "\n",
    "        logger.debug(f\"Saved location {location_id} to Azure Blob Storage: {blob_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving location {location_id} to Azure Blob Storage: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb019ad-298c-4d24-bbee-1c88b8503e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_sensor_to_blob(sensor, is_active=True):\n",
    "    '''\n",
    "    Saves a single sensor as an individual JSON file in Azure Blob Storage.\n",
    "\n",
    "    Args:\n",
    "        sensor (dict): Sensor data to be saved.\n",
    "        is_active (bool, optional): Indicates if the sensor is active. Default=True.\n",
    "    '''\n",
    "    try:\n",
    "        sensor_id = str(sensor[\"sensor_id\"])\n",
    "        parameter = sensor[\"parameter\"].lower()\n",
    "        location_id = str(sensor[\"location_id\"])\n",
    "        \n",
    "        sensor_data = {\n",
    "            \"sensor_id\": sensor_id,\n",
    "            \"location_id\": location_id,\n",
    "            \"parameter\": parameter,\n",
    "            \"is_active\": is_active\n",
    "        }\n",
    "\n",
    "        blob_name = f\"{SENSORS_PATH}/sensor_{parameter}_{sensor_id}.json\"\n",
    "        container_client.upload_blob(blob_name, json.dumps(sensor, indent=2), overwrite=True)\n",
    "\n",
    "        logger.debug(f\"Saved sensor {sensor_id} ({parameter}) to Azure Blob Storage: {blob_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving sensor {sensor_id} ({parameter}) to Azure Blob Storage: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f40d12-817d-4fdc-8fa9-14cd3e79dccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a global HTTP session for connection optimization.\n",
    "'''\n",
    "\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd87d22-be87-4ce2-9ebf-e22e3afae04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_locations(existing_locations):\n",
    "    '''\n",
    "    Fetches new location data from the OpenAQ API.\n",
    "\n",
    "    Args:\n",
    "        existing_locations (set): Set of stored location IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of new locations.\n",
    "    '''\n",
    "    retry_attempts = 3\n",
    "    new_locations = []\n",
    "\n",
    "    while retry_attempts > 0:\n",
    "        try:\n",
    "            logger.info(\"Fetching new location data from OpenAQ API.\")\n",
    "\n",
    "            # Constructing the request URL for the OpenAQ API.\n",
    "            url = f\"{API_LOCATIONS_URL}?iso={COUNTRY_CODE}&limit={PAGE_LIMIT}\"\n",
    "            # Sending a GET request to the API with an authorization key.\n",
    "            response = session.get(url, headers={\"X-API-Key\": API_KEY_OPENAQ}, timeout=REQUEST_TIMEOUT)\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                retry_attempts -= 1\n",
    "                if retry_attempts == 0:  \n",
    "                    logger.error(\"Rate limit (429) while fetching locations exceeded max retries. Skipping.\")\n",
    "                    break\n",
    "                logger.warning(\"Rate limit hit (429) while fetching locations. Retrying in 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            results = response.json().get(\"results\", [])\n",
    "\n",
    "            # Create a set of location IDs returned by the API.\n",
    "            api_location_ids = {str(loc.get(\"id\")) for loc in results}\n",
    "            missing_ids = location_ids - api_location_ids\n",
    "\n",
    "            for loc_id in missing_ids:\n",
    "                logger.warning(f\"Location {loc_id} is in `location_ids` but not in API response!\")\n",
    "\n",
    "            for loc in results:\n",
    "                if str(loc.get(\"id\")) in location_ids and str(loc.get(\"id\")) not in existing_locations:\n",
    "                    new_locations.append(loc)\n",
    "\n",
    "            logger.info(f\"Retrieved {len(new_locations)} new locations.\")\n",
    "            return new_locations  \n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.exception(f\"Error fetching location data: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb14d6c-ab1a-4fc5-85f0-a8080d1a98bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_locations(existing_locations):\n",
    "    '''\n",
    "    Fetches and saves new locations to Azure Blob Storage.\n",
    "    Deletes JSON files of locations that were removed from 'location_ids'.\n",
    "\n",
    "    Args:\n",
    "        existing_locations (set): Set of location IDs currently stored in Azure Blob Storage.\n",
    "    '''\n",
    "    removed_locations = existing_locations - location_ids\n",
    "    saved_locations = []\n",
    "\n",
    "    for loc_id in removed_locations:\n",
    "        blob_name = f\"{LOCATIONS_PATH}/location_{loc_id}.json\"\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        try:\n",
    "            location_data = json.loads(blob_client.download_blob().readall())\n",
    "            \n",
    "            if not location_data.get(\"is_active\", True):\n",
    "                continue\n",
    "\n",
    "            location_data[\"is_active\"] = False\n",
    "            save_location_to_blob(location_data, is_active=False)\n",
    "            logger.warning(f\"Marked location {loc_id} as inactive in Azure Blob Storage.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error marking location {loc_id} as inactive: {e}\")\n",
    "\n",
    "    new_locations = fetch_locations(existing_locations)\n",
    "\n",
    "    for location in new_locations:\n",
    "        save_location_to_blob(location, is_active=True)\n",
    "        saved_locations.append(location[\"id\"])\n",
    "\n",
    "    if saved_locations:\n",
    "        logger.info(f\"Saved {len(saved_locations)} new locations to Azure Blob Storage: {sorted(saved_locations)}\")\n",
    "    else:\n",
    "        logger.info(\"No new locations saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b91c57-5887-4b18-a585-3651bbde8b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_locations_from_blob():\n",
    "    '''\n",
    "    Loads location data from Azure Blob Storage by reading every JSON file.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of location IDs.\n",
    "    '''\n",
    "    all_locations = set()\n",
    "\n",
    "    try:\n",
    "        blob_list = list(container_client.list_blobs(name_starts_with=LOCATIONS_PATH))\n",
    "\n",
    "        if not blob_list:\n",
    "            logger.info(f\"No location data found in Azure Blob Storage for path: {LOCATIONS_PATH}\")\n",
    "            return all_locations\n",
    "\n",
    "        for blob in blob_list:\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = json.loads(blob_client.download_blob().readall())\n",
    "\n",
    "            for loc in blob_data:\n",
    "                if \"id\" in loc:\n",
    "                    all_locations.add(str(loc[\"id\"]))\n",
    "\n",
    "        logger.info(f\"Loaded {len(all_locations)} location IDs from all JSON files.\")\n",
    "        return all_locations\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error retrieving all location data from {LOCATIONS_PATH}: {e}\")\n",
    "        return all_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53439304-3306-4d3c-88b9-8d78a253491d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_sensors_from_blob():\n",
    "    \"\"\"\n",
    "    Loads all sensor data from Azure Blob Storage by reading every JSON file.\n",
    "\n",
    "    Returns:\n",
    "        set: Set of sensor IDs.\n",
    "    \"\"\"\n",
    "    all_sensors = set()\n",
    "\n",
    "    try:\n",
    "        blob_list = list(container_client.list_blobs(name_starts_with=SENSORS_PATH))\n",
    "\n",
    "        if not blob_list:\n",
    "            logger.info(f\"No sensor data found in Azure Blob Storage for path: {SENSORS_PATH}\")\n",
    "            return all_sensors\n",
    "\n",
    "        for blob in blob_list:\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = json.loads(blob_client.download_blob().readall())\n",
    "\n",
    "            for sensor in blob_data:\n",
    "                if \"sensor_id\" in sensor:\n",
    "                    all_sensors.add(str(sensor[\"sensor_id\"]))\n",
    "\n",
    "        logger.info(f\"Loaded {len(all_sensors)} sensor IDs from all JSON files.\")\n",
    "        return all_sensors\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error retrieving all sensor data from {SENSORS_PATH}: {e}\")\n",
    "        return all_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d48480-8805-4901-8b81-41de0b453d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_sensors_for_all_locations(existing_sensors):\n",
    "    '''\n",
    "    Fetches new sensor data for locations.\n",
    "\n",
    "    Args:\n",
    "        existing_sensors (set): Set of stored sensor IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of new sensors.\n",
    "    '''\n",
    "    all_sensors = []\n",
    "    existing_sensor_ids = {str(s) for s in existing_sensors}\n",
    "\n",
    "    for loc_id in location_ids:\n",
    "        retry_attempts = 3\n",
    "\n",
    "        while retry_attempts > 0:\n",
    "            try:\n",
    "                url = f\"{API_LOCATIONS_URL}/{loc_id}\"\n",
    "                response = session.get(url, headers={\"X-API-Key\": API_KEY_OPENAQ}, timeout=REQUEST_TIMEOUT)\n",
    "\n",
    "                if response.status_code == 429:\n",
    "                    retry_attempts -= 1\n",
    "                    if retry_attempts == 0:  \n",
    "                        logger.error(f\"Rate limit (429) for location {loc_id} exceeded max retries. Skipping.\")\n",
    "                        break\n",
    "                    logger.warning(f\"Rate limit hit (429) for location {loc_id}. Retrying in 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "                location_data = response.json().get(\"results\", [])\n",
    "                if not location_data:\n",
    "                    logger.warning(f\"No data found for location {loc_id}.\")\n",
    "                    break\n",
    "\n",
    "                sensors = location_data[0].get(\"sensors\", [])\n",
    "\n",
    "                new_sensors = [\n",
    "                    {\n",
    "                        \"sensor_id\": s[\"id\"],\n",
    "                        \"parameter\": s[\"parameter\"][\"name\"].lower(),\n",
    "                        \"location_id\": loc_id\n",
    "                    }\n",
    "                    for s in sensors\n",
    "                    if s.get(\"parameter\", {}).get(\"name\", \"\").lower() in air_quality_parameters\n",
    "                    and str(s[\"id\"]) not in existing_sensor_ids\n",
    "                ]\n",
    "\n",
    "                if new_sensors:\n",
    "                    all_sensors.extend(new_sensors)\n",
    "\n",
    "                break\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Error fetching sensors for location {loc_id}: {e}\")\n",
    "                break  \n",
    "\n",
    "    logger.info(f\"Retrieved {len(all_sensors)} new sensors.\")\n",
    "    return all_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20409ebe-384d-4624-908e-236ef503966e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_sensors(existing_sensors, existing_locations):\n",
    "    '''\n",
    "    Fetches and saves only new sensors to Azure Blob Storage.\n",
    "    Deletes only sensors that belong to removed locations.\n",
    "\n",
    "    Args:\n",
    "        existing_sensors (dict): Dictionary mapping sensor IDs to their respective Azure Blob Storage file paths.\n",
    "        existing_locations (set): Set of all location IDs currently stored in Azure Blob Storage.\n",
    "    '''\n",
    "    removed_sensors = []\n",
    "    saved_sensors = []\n",
    "\n",
    "    for sensor_id, sensor_file in list(existing_sensors.items()):\n",
    "        blob_client = container_client.get_blob_client(sensor_file)\n",
    "        try:\n",
    "            sensor_data = json.loads(blob_client.download_blob().readall())\n",
    "            sensor_location_id = str(sensor_data.get(\"location_id\", \"\"))\n",
    "\n",
    "            if sensor_location_id not in location_ids:\n",
    "                if not sensor_data.get(\"is_active\", True):\n",
    "                    continue\n",
    "                \n",
    "                sensor_data[\"is_active\"] = False\n",
    "                save_sensor_to_blob(sensor_data, is_active=False)\n",
    "                removed_sensors.append(sensor_id)\n",
    "                logger.warning(f\"Marked sensor {sensor_id} as inactive in Azure Blob Storage.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error marking sensor {sensor_id} as inactive: {e}\")\n",
    "\n",
    "    new_sensors = fetch_sensors_for_all_locations(existing_sensors.keys())\n",
    "\n",
    "    for sensor in new_sensors:\n",
    "        save_sensor_to_blob(sensor, is_active=True)\n",
    "        saved_sensors.append(sensor[\"sensor_id\"])\n",
    "\n",
    "    if saved_sensors:\n",
    "        logger.info(f\"Saved {len(saved_sensors)} new sensors: {sorted(saved_sensors)}\")\n",
    "    else:\n",
    "        logger.info(\"No new sensors saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb9f75b-28b0-4b9f-830a-62d9efd9a7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    existing_location_files = list(container_client.list_blobs(name_starts_with=LOCATIONS_PATH))\n",
    "    existing_location_ids = {blob.name.split(\"_\")[-1].split(\".\")[0] for blob in existing_location_files}\n",
    "\n",
    "    process_locations(existing_location_ids)\n",
    "\n",
    "    existing_sensor_files = list(container_client.list_blobs(name_starts_with=SENSORS_PATH))\n",
    "    existing_sensor_data = {blob.name.split(\"_\")[-1].split(\".\")[0]: blob.name for blob in existing_sensor_files}\n",
    "\n",
    "    process_sensors(existing_sensor_data, existing_location_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd6ea48-eed2-4af3-b732-357a036b1ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Data Ingestion: Locations and Sensors",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}